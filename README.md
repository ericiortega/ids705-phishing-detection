# Behavioral Cues and Adversarial Robustness in Phishing Email Detection

### Final project for Duke University's IDS 705: Machine Learning course.

**Team Capybara:** Ailina Aniwan | Jay Liu | Eric Ortega Rodriguez | Tursunai Turumbekova

## ðŸ“Œ Overview

This project explores phishing email classification using both traditional machine learning and deep learning techniques, aiming to evaluate model accuracy, interpretability, robustness, and feature enhancement strategies.

## ðŸ“„ Abstract

Phishing emails remain a persistent and evolving cybersecurity threat, often exploiting linguistic ambiguity and psychological manipulation to bypass conventional filters. This project explores phishing detection through the lens of supervised learning by analyzing four key dimensions: model architecture, text preprocessing, adversarial robustness, and behavioral feature engineering. We compare traditional machine learning models (NaÃ¯ve Bayes, Logistic Regression, XGBoost) with deep learning approaches (BERT), and assess how incorporating structured metadata and psychological deception cues influences classification performance.

Our findings indicate that BERT delivers consistently high performance with minimal parameter tuning, while XGBoost combined with engineered features achieves the highest overall accuracy. Importantly, we demonstrate that reducing false negativesâ€”even marginallyâ€”can significantly improve security in high-risk environments. Lastly, we show that integrating behavioral deception features enhances performance in lightweight models, offering a practical and interpretable alternative or complement to deep content-based approaches.



## ðŸ“‚ Repository Structure

- **`data/`**: Contains the TREC 2007 Public Spam Corpus dataset.
- **`notebooks/`**: Jupyter notebooks: model training and comparison Experiment 1, data preprocessing pipelines Experiment 2, adverserial noise model evaluation Experiment 3, and Social Deception features evaluation Experiment 4
- **`results/`**: Stored outputs including model performance summary
- **`requirements.txt`**: List of Python dependencies required to run the project.
- **`README.md`**
.
```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # Raw TREC 2007 Public Spam Corpus
â”‚   â””â”€â”€ cleaning/        # Data cleaning notebook and cleaned CSV
â”œâ”€â”€ notebooks/           # Experiments 1, 2, 3, 4
â”œâ”€â”€ results/             # Output files 
â”œâ”€â”€ requirements.txt     # Project dependencies
â””â”€â”€ README.md            # Project documentation
```

## ðŸ“š Data

We utilized the **2007 TREC Public Spam Corpus**, a publicly available dataset hosted on Zenodo, as curated by Champa et al. (2024b). The dataset includes **52,713 labeled emails**, each annotated as either:

- **Phishing (label = 1)**  
- **Not phishing (label = 0)**

Each email entry contains rich metadata fields such as:

- Sender and receiver addresses  
- Subject line  
- Email body  
- URL counts  
- Timestamp information

This structure enabled us to experiment with both **pure-text approaches** (e.g., BERT, TF-IDF + Logistic Regression) and **hybrid models** incorporating behavioral or structural features.

> âš ï¸ **Limitations**:  
> While the dataset offers a realistic view of email-based phishing strategies as of 2007, phishing tactics have evolved significantly. Modern attacks now incorporate AI-generated content, targeted spear-phishing, and advanced evasion strategiesâ€”often personalized for specific victims (IBM X-Force Threat Intelligence Index, 2024). Thus, while our models perform well on this corpus, real-world generalization may require retraining on newer datasets.

As shown in Figure 1 of our notebook, the dataset exhibits **moderate class imbalance**, with phishing emails making up approximately **54%** of all entries. This imbalance underscores the need to optimize for **recall**, as false negatives in phishing detection carry significant security risks.



## ðŸ§ª Models Implemented

The project explores and compares the following models:

### Traditional Machine Learning Models

- **Logistic Regression**
- **NaÃ¯ve Bayes**
- **Random Forest**
- **XGBoost**

### Deep Learning Models

- **BERT (Bidirectional Encoder Representations from Transformers)**

## ðŸ“Š Evaluation Metrics

To assess model performance, we utilize:

- **Accuracy**
- **Precision**
- **Recall**
- **F1 Score**
- **Confusion Matrix**

These metrics provide a comprehensive view of each model's effectiveness in classifying phishing emails.

## ðŸ“ˆ Key Findings

- BERT consistently outperforms all models with minimal tuning and strong generalization.

- XGBoost achieves the highest overall accuracy when enhanced with structured and behavioral features.

- Behavioral deception cues (e.g., urgency, fear) enhance performance in lightweight models like Logistic Regression.

- Adversarial perturbations challenge BERTâ€™s robustness, motivating further research in adversarial training. 

### Prerequisites

Ensure you have Python 3.7 or higher installed. Then, install the required packages:

```bash
pip install -r requirements.txt
```

## ðŸ“š References

- Champa, A. I., Zibran, M. F., & Rahayu, W. (2024a). *Why phishing emails escape detection: A closer look at the failure points.* 2024 12th International Symposium on Digital Forensics and Security (ISDFS), IEEE. [IEEE Link](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10527344)

- Champa, A. I., Zibran, M. F., & Rahayu, W. (2024b). *Curated datasets and feature analysis for phishing email detection with machine learning.* 2024 3rd IEEE International Conference on Computing and Machine Intelligence (ICMI). [Paper PDF](https://www2.cose.isu.edu/~minhazzibran/resources/MyPapers/Champa_ICMI2024_Published.pdf)

